{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction:\n",
    "\n",
    "This notebook demonstrates -\n",
    "\n",
    "1. Use of DPU and Preprocessing Accelerators to perform AI Inference on video input.\n",
    "2. Option to run different AI models like facedetect, refinedet, and SSD using the same set of accelerators.\n",
    "\n",
    "The application is based on the VVAS (Vitis Video Analytics SDK) framework, also utilizing the open source GStreamer plugins.\n",
    "\n",
    "Vitis Video Analytics SDK (VVAS) is developed by Xilinx to provide many useful GStreamer plugins as the middleware between the application and underlying FPGA accelerators, including DPU AI inference engine, and other PL accelerators such as the one for AI input preprocessing.\n",
    "\n",
    "Please refer to the [Kria™ KV260 Vision AI Starter Kit Applications GitHub Pages](https://xilinx.github.io/kria-apps-docs/index.html) for detailed HW/SW architecture and [Vitis Video Analytics SDK GitHub Pages](https://xilinx.github.io/VVAS/#) for the VVAS related info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "from IPython.display import *\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import *\n",
    "import time       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Video Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline flow-\n",
    "\n",
    "The VCU decode the H264 file to NV12 format.\n",
    "The preprocessing block resizes, quantizes, and converts to BGR format.\n",
    "DPU does the AI inference based on the model selected and generates bounding box data.\n",
    "Meta Affixer scales the bounding box data recieved wrt the resolution of vcu decoded output.\n",
    "The bounding box draws the results around the objects of interest.\n",
    "\n",
    "PL - Pre-proc, DPU\n",
    "\n",
    "Hardened Block - VCU Dec, Display Port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='./images/ml-inference.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If DPU is in slot0 and PP_PIPELINE is in slot1. Copy the dpu_slot0 xclbin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp /lib/firmware/xilinx/k26-dfx-2rp/DPU/DPU_slot0/dpu_RP_0.xclbin /lib/firmware/xilinx/k26-dfx-2rp/DPU\n",
    "! mv /lib/firmware/xilinx/k26-dfx-2rp/DPU/dpu_RP_0.xclbin /lib/firmware/xilinx/k26-dfx-2rp/DPU/dfx-ml-inference.xclbin\n",
    "! cp /lib/firmware/xilinx/k26-dfx-2rp/DPU/DPU_slot0/dpu_RP_0.xclbin /usr/lib\n",
    "! mv /usr/lib/dpu_RP_0.xclbin /usr/lib/dpu.xclbin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If DPU is in slot1 and PP_PIPELINE is in slot0. Copy the dpu_slot1 xclbin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp /lib/firmware/xilinx/k26-dfx-2rp/DPU/DPU_slot1/dpu_RP_1.xclbin /lib/firmware/xilinx/k26-dfx-2rp/DPU\n",
    "! mv /lib/firmware/xilinx/k26-dfx-2rp/DPU/dpu_RP_1.xclbin /lib/firmware/xilinx/k26-dfx-2rp/DPU/dfx-ml-inference.xclbin\n",
    "! cp /lib/firmware/xilinx/k26-dfx-2rp/DPU/DPU_slot1/dpu_RP_1.xclbin /usr/lib\n",
    "! mv /usr/lib/dpu_RP_1.xclbin /usr/lib/dpu.xclbin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Choose AI Inference Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Option to set required AI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aitask= [ \"refinedet\",\"facedetect\",\"ssd\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = widgets.Dropdown(options=aitask,value=aitask[0],description='ai_task :',)\n",
    "ai_task = aitask[0]\n",
    "def on_change(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        global ai_task \n",
    "        ai_task = change['new']\n",
    "w.observe(on_change)\n",
    "display(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Running Input media Sample Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ai_task == 'ssd':\n",
    "    src = './Road-Adas.nv12.h264'\n",
    "else:\n",
    "    src = './walking-people.nv12.h264'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gst-launch-1.0 filesrc location={src} ! h264parse ! omxh264dec !  tee name=t0 t0.src_0 !  queue !  kmssink bus-id=fd4a0000.display  sync=false fullscreen-overlay=true > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Run AI Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "confdir=\"/opt/xilinx/dfx-ml-inference/vvas/config/\"+ ai_task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bounding box will come on the region of interest on the monitor depending upon the chosen model. \n",
    "\n",
    "Pipeline running on Input file-\n",
    "\n",
    "gst-launch-1.0 filesrc location={src} ! h264parse ! omxh264dec ! tee name=t0 ! queue ! vvas_xmultisrc kconfig=\"{confdir}/preprocess.json\" ! vvas_xfilter kernels-config=\"{confdir}/aiinference.json\" ! scalem0.sink_master vvas_xmetaaffixer name=scalem0 scalem0.src_master ! fakesink t0. ! queue max-size-buffers=1 leaky=2 ! scalem0.sink_slave_0 scalem0.src_slave_0 ! queue ! vvas_xfilter kernels-config=\"{confdir}/drawresult.json\" ! queue ! kmssink  bus-id=fd4a0000.display sync=false fullscreen-overlay=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gst-launch-1.0 filesrc location={src} ! h264parse ! omxh264dec ! tee name=t0 ! queue ! vvas_xmultisrc kconfig=\"{confdir}/preprocess.json\" ! vvas_xfilter kernels-config=\"{confdir}/aiinference.json\" ! scalem0.sink_master vvas_xmetaaffixer name=scalem0 scalem0.src_master ! fakesink t0. ! queue max-size-buffers=1 leaky=2 ! scalem0.sink_slave_0 scalem0.src_slave_0 ! queue ! vvas_xfilter kernels-config=\"{confdir}/drawresult.json\" ! queue ! kmssink bus-id=fd4a0000.display sync=false fullscreen-overlay=true > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Summary\n",
    "The Jupyter application shows how to use two accelerators-DPU and PP_PIPELINE in different slots to do inference on the incoming frames, and draw boundboxing of detected results.\n",
    "Both accelerators work in both slots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Copyright© 2023 Xilinx</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
